# Sentiment Analysis with LLM Embeddings üé≠

**Groupe 10** - Projet Acad√©mique de Classification de Sentiments sur Twitter

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/ü§ó-Transformers-yellow.svg)](https://huggingface.co/transformers/)

## üìã Table des Mati√®res

- [Vue d'ensemble](#-vue-densemble)
- [Dataset](#-dataset)
- [Architecture du Projet](#-architecture-du-projet)
- [Approches et Mod√®les](#-approches-et-mod√®les)
- [R√©sultats](#-r√©sultats)
- [Installation](#-installation)
- [Utilisation](#-utilisation)
- [Structure des Fichiers](#-structure-des-fichiers)
- [Contributeurs](#-contributeurs)
- [R√©f√©rences](#-r√©f√©rences)

## üéØ Vue d'ensemble

Ce projet acad√©mique explore diff√©rentes approches pour la **classification de sentiments** sur des tweets en trois cat√©gories : **positif**, **n√©gatif** et **neutre**. Nous comparons des m√©thodes traditionnelles de machine learning, des r√©seaux de neurones MLP, et des mod√®les de langage pr√©-entra√Æn√©s (LLMs) avec fine-tuning.

### Objectifs principaux

1. **Comparer** les performances de diff√©rentes approches de classification
2. **√âvaluer** l'apport des embeddings BERT vs vectorisation classique (TF-IDF)
3. **Explorer** le fine-tuning l√©ger (LoRA) de mod√®les Transformers
4. **Impl√©menter** une approche d'ensemble (voting) pour am√©liorer les performances

## üìä Dataset

- **Source**: [Sentiment Analysis Dataset - Kaggle](https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset)
- **Type**: Tweets avec m√©tadonn√©es
- **Classes**: 3 cat√©gories de sentiment
  - N√©gatif (0)
  - Neutre (1)
  - Positif (2)
- **Taille**: ~27,000 tweets d'entra√Ænement
- **Features suppl√©mentaires**: Time of Tweet, Age of User, donn√©es d√©mographiques

## üèóÔ∏è Architecture du Projet


```
 Pr√©traitement et exploration des donn√©es
    ‚îú‚îÄ‚îÄ Nettoyage et tokenization
    ‚îú‚îÄ‚îÄ Vectorisation TF-IDF
    ‚îî‚îÄ‚îÄ Feature engineering
    
 Mod√®les classiques de ML
    ‚îú‚îÄ‚îÄ LinearSVC
    ‚îú‚îÄ‚îÄ Logistic Regression
    ‚îú‚îÄ‚îÄ Multinomial Naive Bayes
    ‚îî‚îÄ‚îÄ Random Forest
    
R√©seaux de neurones (MLP)
    ‚îú‚îÄ‚îÄ MLP sur TF-IDF
    ‚îî‚îÄ‚îÄ MLP sur embeddings BERT
    
 Analyse comparative

 LLM pr√©-entra√Æn√© (DistilBERT)

 Analyse BERT et embeddings

 Fine-tuning avec LoRA
    ‚îú‚îÄ‚îÄ RoBERTa-base + LoRA
    ‚îú‚îÄ‚îÄ BERT-base + LoRA
    ‚îú‚îÄ‚îÄ DistilBERT + LoRA
    ‚îî‚îÄ‚îÄ Ensemble Voting
```

##  Approches et Mod√®les

### 1. Machine Learning Classique

Vectorisation TF-IDF + mod√®les traditionnels :

| Mod√®le | Train F1 | Val F1 | Test F1 |
|--------|----------|--------|---------|
| LinearSVC | 0.725 | 0.713 | 0.714 |
| Logistic Regression | 0.730 | 0.709 | 0.711 |
| Multinomial NB | 0.669 | 0.660 | 0.661 |
| Random Forest | 0.972 | 0.697 | 0.696 |

**Meilleur mod√®le classique**: Logistic Regression


#### MLP sur TF-IDF
- **Architecture**: R√©seau dense √† plusieurs couches
- **Performance**: F1-score ~0.73 (similaire aux mod√®les classiques)

#### MLP sur Embeddings BERT
- **Simple Classifier** (1 couche lin√©aire): 65.17% accuracy
- **MLP Classifier** (3 couches): 67.23% accuracy
- **Am√©lioration**: Les embeddings BERT capturent mieux la s√©mantique

### 3. Fine-tuning avec LoRA

Utilisation de **LoRA (Low-Rank Adaptation)** pour un fine-tuning efficace :

| Mod√®le | Params entra√Ænables | Train Loss | Val F1 | Test F1 | Test Accuracy |
|--------|---------------------|------------|--------|---------|---------------|
| **RoBERTa-base** | 1.48M (1.10%) | 0.4269 | 0.7913 | **0.7948** | 79.49% |
| **BERT-base** | 887K (0.80%) | 0.4436 | 0.7943 | **0.7947** | 79.46% |
| **DistilBERT** | 740K (1.09%) | 0.4859 | 0.7921 | **0.7860** | 78.58% |

**Configuration LoRA**:
- `r=8` (rank)
- `lora_alpha=16`
- `lora_dropout=0.1`
- Appliqu√© sur: `query` et `value` layers

### 4. Ensemble Voting 

Combinaison pond√©r√©e des 3 mod√®les LLM :

```python
Weights: RoBERTa (0.50) + BERT (0.35) + DistilBERT (0.15)
```

**R√©sultats Ensemble**:
- **F1-Score**: 0.8069 ( 1.21% vs meilleur mod√®le individuel)
- **Accuracy**: 80.67%

##  R√©sultats

### Comparaison Finale

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Mod√®le                      ‚îÇ Train F1  ‚îÇ  Val F1   ‚îÇ  Test F1  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ LinearSVC (TF-IDF)          ‚îÇ   0.725   ‚îÇ   0.713   ‚îÇ   0.714   ‚îÇ
‚îÇ Logistic Regression         ‚îÇ   0.730   ‚îÇ   0.709   ‚îÇ   0.711   ‚îÇ
‚îÇ Random Forest               ‚îÇ   0.972   ‚îÇ   0.697   ‚îÇ   0.696   ‚îÇ
‚îÇ MLP (TF-IDF)                ‚îÇ   0.733   ‚îÇ   0.717   ‚îÇ   0.719   ‚îÇ
‚îÇ MLP (BERT embeddings)       ‚îÇ    -      ‚îÇ    -      ‚îÇ   0.672   ‚îÇ
‚îÇ RoBERTa + LoRA              ‚îÇ   0.821   ‚îÇ   0.791   ‚îÇ   0.795   ‚îÇ
‚îÇ BERT + LoRA                 ‚îÇ   0.820   ‚îÇ   0.794   ‚îÇ   0.795   ‚îÇ
‚îÇ DistilBERT + LoRA           ‚îÇ   0.811   ‚îÇ   0.792   ‚îÇ   0.786   ‚îÇ
‚îÇ ENSEMBLE (3 LLMs)           ‚îÇ    -      ‚îÇ    -      ‚îÇ   0.807   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Observations Cl√©s

1. **LLMs >> ML Classique**: Gain de ~8-9% en F1-score
2. **LoRA efficace**: Seulement 0.8-1.1% des param√®tres entra√Æn√©s
3. **Ensemble b√©n√©fique**: +1.2% vs meilleur mod√®le seul
4. **RoBERTa l√©g√®rement meilleur**: Mais √©cart faible avec BERT

## üöÄ Installation

### Pr√©requis

- Python 3.8+
- CUDA (recommand√© pour GPU)
- 8GB+ RAM

### √âtapes d'installation

```bash
# Cloner le repository
git clone https://github.com/votregroupe/sentiment-analysis-llm.git
cd sentiment-analysis-llm

# Cr√©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installer les d√©pendances
pip install -r requirements.txt

# T√©l√©charger les ressources NLTK
python -c "import nltk; nltk.download('stopwords'); nltk.download('punkt')"
```

### Requirements principaux

```
torch>=2.0.0
transformers>=4.30.0
peft>=0.4.0
datasets>=2.12.0
scikit-learn>=1.2.0
pandas>=1.5.0
numpy>=1.23.0
matplotlib>=3.6.0
seaborn>=0.12.0
nltk>=3.8
tqdm>=4.65.0
kagglehub
```

##  Utilisation

### 1. T√©l√©charger le dataset

```python
import kagglehub
path = kagglehub.dataset_download('abhi8923shriv/sentiment-analysis-dataset')
print(f'Dataset t√©l√©charg√© dans: {path}')
```

### 2. Ex√©cuter le notebook

```bash
jupyter notebook Sentiment_Analysis_LLM_Embeddings_Groupe10.ipynb
```

### 3. Pipeline complet

Le notebook est organis√© s√©quentiellement. Ex√©cutez les cellules dans l'ordre pour :

1.  Pr√©traiter les donn√©es
2.  Entra√Æner les mod√®les classiques
3.  Tester les MLPs
4.  Fine-tuner les LLMs avec LoRA
5.  Cr√©er l'ensemble et √©valuer

### 4. Pr√©diction sur un nouveau texte

```python
# Exemple avec RoBERTa fine-tun√©
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Charger le mod√®le
model_name = "roberta-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

# Pr√©dire
text = "I love this product, it works perfectly!"
inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=64)

with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.softmax(outputs.logits, dim=-1)
    sentiment = torch.argmax(predictions, dim=-1).item()

# Afficher le r√©sultat
sentiments = {0: "N√©gatif", 1: "Neutre", 2: "Positif"}
print(f"Sentiment: {sentiments[sentiment]}")
print(f"Confiance: {predictions[0][sentiment]:.2%}")
```

##  M√©thodologie

### Pr√©traitement du Texte

```python
def preprocess_text(text):
    # Nettoyage
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)  # URLs
    text = re.sub(r'@\w+|#\w+', '', text)  # Mentions/Hashtags
    text = re.sub(r'[^\w\s]', '', text)  # Ponctuation
    
    # Tokenization
    tokens = word_tokenize(text)
    
    # Stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [t for t in tokens if t not in stop_words]
    
    return ' '.join(tokens)
```

### Strat√©gie d'entra√Ænement LoRA

```python
from peft import LoraConfig, get_peft_model

# Configuration LoRA
lora_config = LoraConfig(
    r=8,                          # Rank
    lora_alpha=16,                # Scaling factor
    target_modules=["query", "value"],
    lora_dropout=0.1,
    bias="none",
    task_type="SEQ_CLS"
)

# Appliquer LoRA au mod√®le
model = get_peft_model(base_model, lora_config)

# Training
optimizer = AdamW(model.parameters(), lr=2e-5)
epochs = 5
batch_size = 32
```

### Ensemble Voting

```python
def ensemble_prediction(text, models, tokenizers, weights):
    predictions = []
    
    for model, tokenizer in zip(models, tokenizers):
        inputs = tokenizer(text, return_tensors='pt', 
                          padding=True, truncation=True, max_length=64)
        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()[0]
            predictions.append(probs)
    
    # Moyenne pond√©r√©e
    ensemble_probs = sum(w * p for w, p in zip(weights, predictions))
    return np.argmax(ensemble_probs)
```

##  Apprentissages Cl√©s

### Pourquoi BERT embeddings > TF-IDF ?

1. **Repr√©sentation contextuelle**: BERT capture le sens en fonction du contexte
2. **Similarit√© s√©mantique**: Mots similaires ont des embeddings proches
3. **Pr√©-entra√Ænement massif**: Connaissances g√©n√©rales du langage

### Avantages de LoRA

- **Efficacit√© m√©moire**: <1% des param√®tres √† entra√Æner
- **Rapidit√©**: Fine-tuning 3-5x plus rapide
- **Performances**: R√©sultats comparables au fine-tuning complet
- **Flexibilit√©**: Plusieurs adaptateurs pour diff√©rentes t√¢ches

##  Analyses Compl√©mentaires

### Distribution des Classes

```
Classe N√©gative: ~28%
Classe Neutre:   ~40%
Classe Positive: ~32%
```

‚Üí Dataset l√©g√®rement d√©s√©quilibr√© vers la classe neutre

### Matrice de Confusion (Ensemble)

```
              Pr√©diction
           Neg  Neu  Pos
R√©el Neg   450   78   22
     Neu    65  512   73
     Pos    18   89  443
```

**Observations**:
- Meilleure pr√©cision sur les sentiments n√©gatifs et positifs
- Classe neutre plus difficile (confusions avec pos/neg)

##  Contributeurs

- **Mahdi Abid**
- **Mohamed Amine Chaghal**
- **Mohamed Klila**

##  R√©f√©rences

### Papers

- Devlin et al. (2018) - [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- Liu et al. (2019) - [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
- Sanh et al. (2019) - [DistilBERT, a distilled version of BERT](https://arxiv.org/abs/1910.01108)
- Hu et al. (2021) - [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)

### Documentation

- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- [PEFT Library](https://huggingface.co/docs/peft/)
- [PyTorch Documentation](https://pytorch.org/docs/)

### Datasets

- [Sentiment Analysis Dataset - Kaggle](https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset)

##  Remerciements

- **Professeurs et encadrants** pour leurs conseils
- **Hugging Face** pour les mod√®les pr√©-entra√Æn√©s et la biblioth√®que Transformers
- **Kaggle** pour le dataset
- **Communaut√© PyTorch** pour les ressources √©ducatives

  
‚≠ê Si ce projet vous a √©t√© utile, n'h√©sitez pas √† lui donner une √©toile !


